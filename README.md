# Build-your-own-GPT

## Project Overview

This project is an implementation of a Generative Pre-trained Transformer (GPT) model, inspired by the seminal 2017 paper "Attention is All You Need" and guided by Andrej Karpathy's build-your-own-GPT tutorial.

### Project Goals

- Implement the Transformer architecture from scratch
- Understand the core mechanisms of self-attention
- Create a working GPT model for text generation
- Participate in the #365DaysOfCode #DrGViswanathan initiative

## Background

### The Original Paper

The ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. introduced the Transformer architecture, revolutionizing natural language processing by replacing recurrent neural networks with a pure attention mechanism.

### Key Concepts

- Self-Attention Mechanism
- Multi-Head Attention
- Positional Encoding
- Transformer Encoder and Decoder Architectures

## Implementation Details

### Technologies Used
- Python
- PyTorch
- NumPy

## Getting Started

### Prerequisites

- Python 3.8+
- PyTorch
- Basic understanding of deep learning concepts

### Installation

git clone https://github.com/Arihant-Bhandari/Build-your-own-GPT.git
cd Build-your-own-GPT
pip install -r requirements.txt


## Learning Objectives

1. Understand the Transformer architecture
2. Implement self-attention from scratch
3. Build a text generation model
4. Gain hands-on experience with deep learning frameworks

## Resources

- [Original "Attention is All You Need" Paper](https://arxiv.org/abs/1706.03762)
- [Andrej Karpathy's Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [PyTorch Documentation](https://pytorch.org/docs/stable/)

## Acknowledgments

- Inspired by the #365DaysOfCode challenge under the #DrGViswanathan Challenge

- Special thanks to Andrej Karpathy for his educational content
